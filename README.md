# HW1: Learning to Identify High Betweenness Nodes
contributed by < `erickuo5124` >
###### tags: `MLG`
### ä½œæ¥­èªªæ˜
çµ¦å®šä¸€å€‹ networkï¼Œé€é GNN æ‰¾å‡ºåœ–ä¸­ BC(Betweenness Centrality) è¼ƒé«˜çš„é»ï¼Œä¸¦é”åˆ°ä»¥ä¸‹è¦æ±‚ï¼š

- è¨ˆç®—å‡ºå‰ N% é»çš„æ­£ç¢ºç‡
- ç®—å‡ºé æ¸¬æ‰€éœ€æ™‚é–“

### BC (Betweenness Centrality)
åœ¨åœ–è«–ä¸­ï¼Œæ‰€æœ‰æœ€çŸ­è·¯å¾‘ (All-Pairs Shortest Paths) ç©¿è¶Šç¯€é» $v$ çš„æ•¸é‡å³ç‚ºè©²ç¯€é»çš„ $v$ Betweenness Centralityï¼Œè¨ˆç®—æ–¹å¼å¦‚ä¸‹ï¼š

$$
g(v)=\sum_{s\neq v\neq t}\frac{\sigma_{st}(v)}{\sigma_{st}}
$$

å…¶ä¸­ $\sigma_{st}$ ç‚ºç¯€é» $s$ åˆ°ç¯€é» $t$ çš„æœ€çŸ­è·¯å¾‘æ•¸é‡ï¼Œ$\sigma_{st}(v)$ ç‚ºç¶“éç¯€é» $v$ è·¯å¾‘çš„æ•¸é‡ã€‚

è¨ˆç®—æ‰€æœ‰ç¯€é»çš„ Betweenness Centrality éœ€è¦è¨ˆç®—æ‰€æœ‰æœ€çŸ­è·¯å¾‘ï¼Œç›®å‰æœ€çŸ¥åçš„è¨ˆç®—æ–¹å¼ç‚º [Brandes algorithm](http://www.uvm.edu/pdodds/research/papers/others/2001/brandes2001a.pdf)ï¼Œæ™‚é–“è¤‡é›œåº¦åœ¨ unweighted networks ç‚º $O(|V||E|)$ã€‚

:::info
è‹¥æ˜¯ç¯€é»è¢«å¾ˆå¤šæœ€çŸ­è·¯å¾‘ç¶“éï¼Œè©²ç¯€é»çš„ Betweenness Centrality å°±è¶Šé«˜ï¼Œåœ¨ç¶²è·¯ä¸­æ“”ä»»çš„è§’è‰²å°±ç›¸å°é‡è¦ï¼Œæ‡‰è©²å„ªå…ˆè¢«ä¿è­·æˆ–æ‘§æ¯€ï¼Œé€²è€Œæ§åˆ¶ç¶²è·¯çš„å‚³éæ•ˆç‡ã€‚
:::

### Dataset

- Synthetic Data (generate by [powerlaw_cluster_graph](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.powerlaw_cluster_graph.html#networkx.generators.random_graphs.powerlaw_cluster_graph))
- Real world data (com-Youtube)

---
## DrBC
- [github](https://github.com/erickuo5124/MLG_HW1/blob/main/DrBC.ipynb)
- [Google colab](https://colab.research.google.com/drive/1XR6pDJ9WEfs7QOPKYuNgga1s38nOPMOH?usp=sharing)
### ç’°å¢ƒ

#### python ç‰ˆæœ¬
```
Python 3.7.10
```

#### å¥—ä»¶
```shell=
torch 1.8.0+cu101
torch-geometric 1.6.3 
networkx 2.5
```

### æ­£ç¢ºç‡
- Top-1%
```shell

```
- Top-5%
- Top-10%

### Hyper-parameter

|batch-size|embeding-dimension|learning-rate|layer|episodes|
|-|-|-|-|-|
|16|128|0.01|5|100|

----

### æ¨¡å‹å¯¦ä½œ

DrBC çš„å¯¦ä½œä½¿ç”¨ encoder-decoder frameworkï¼Œå°‡æ¯å€‹ç¯€é»ç”¨ encoder æŠ•å½±åˆ°ç©ºé–“ä¸­ï¼ŒBetweenness Centrality ç›¸ä¼¼çš„ç¯€é»åœ¨ç©ºé–“ä¸­ä¹Ÿæœƒæ¯”è¼ƒæ¥è¿‘ã€‚å†å°‡ç©ºé–“ä¸­çš„é»åˆ©ç”¨ decoder é‡åŒ–æˆä¸€å€‹æ•¸å€¼ï¼Œè©²æ•¸å€¼åæ˜ ç¯€é» Betweennes Centrality åœ¨æ‰€æœ‰ç¯€é»ä¸­çš„ç›¸å°å¤§å°ã€‚

#### Network Embedding

æŠŠæ¯å€‹ç¯€é»æŠ•å½±åˆ°ä¸‰ç¶­ç©ºé–“ï¼Œå°‡ initial feature $X_v$ è¨­ç‚ºï¼š

$$
X_v=[d_v,1,1]
$$

å…¶ä¸­ $d_v$ ç‚ºç¯€é» $v$ çš„ [degree](https://zh.wikipedia.org/wiki/%E5%BA%A6_(%E5%9B%BE%E8%AE%BA))ï¼Œä¸” $v$ çš„ç¬¬0å±¤ hidden layer $h^{(0)}_v$ å³ç‚º $X_v$

#### Encoder - Neighborhood Aggregation

DrBC è«–æ–‡ä¸­ä½¿ç”¨ weighted sum aggregator ä¾† aggregate é„°å±…ï¼Œå°‡é€™äº›è³‡è¨Šåš embeddingï¼Œè€Œæ­£å¥½èˆ‡ pytorch geometric ä¸­çš„ [GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv) é¡ä¼¼ï¼Œå‡½å¼å¯¦ä½œå¦‚ä¸‹ï¼š

$$
x'_i=\Theta\sum_{j\in N(v)\cup\{i\}}\frac{e_{j,i}}{\sqrt{\hat{d_j}\hat{d_i}}}x_j
$$

Neighborhood Aggregation å°‡é„°å±…çš„è³‡è¨Šèšåˆé€²ç¯€é»ç•¶ä¸­ï¼Œç•¶ç¥ç¶“ç¶²è·¯ç–Šå¾—è¶Šå¤šå±¤ï¼Œå°±èƒ½å¾—åˆ°é›¢ç¯€é»è¶Šé çš„è³‡è¨Šã€‚

:::warning
GCNConv èˆ‡è«–æ–‡ä¸­çš„è¨ˆç®—æ–¹å¼æœ‰äº›å·®ç•°ï¼Œåœ¨æƒ³å¯èƒ½æ˜¯é€™è£¡æœ‰å•é¡Œ
:::

#### Encoder - COMBINE Function

ç‚ºäº†å¾—åˆ°ç¯€é» $v$ åœ¨ç¬¬ $l$ å±¤çš„ embeddingï¼Œå°‡ $v$ ä¸Šä¸€å±¤çš„ embedding $h^{l-1}_v$ èˆ‡ $v$ æ‰€æœ‰é„°å±… $N(v)$ åœ¨ $l$ å±¤çš„èšåˆ $h^{l}_{N(v)}$ åŠ èµ·ä¾†ï¼Œä½¿ç”¨åˆ°çš„ COMBINE Funtion æ˜¯ [GRUCell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html)ï¼Œè®“æ¨¡å‹å¯ä»¥æ±ºå®šå¤šé è·é›¢é„°å±…çš„ feature å¯ä»¥åŠ åˆ°ä¸‹ä¸€å±¤å…§ã€‚

#### Encoder - Layer Aggregation

ä½¿ç”¨ max-pooling aggregatorï¼Œåœ¨æ¯å€‹ç¶­åº¦é¸æ“‡æœ€å¤§çš„ feature å¯ä»¥è®“æˆ‘å€‘å¾—åˆ°è³‡è¨Šæœ€å¤šçš„ layerï¼Œå­˜ç‚º $z$ å³ç‚ºæˆ‘å€‘è¦çš„ embedding çš„ Betweenness Centralityã€‚

----

#### Decoder

ç”¨å…©å±¤çš„ Linear ç®—å‡ºç¯€é»çš„ BC ranking scoreï¼š
$$
y=W_5ReLU(W_4z)
$$

#### Pairwise Ranking Loss
é æ¸¬å‡ºä¾†çš„å€¼æœƒæ˜¯ BC ranking scoreï¼Œä½†å¯¦éš›ç›®æ¨™ä¸¦ä¸æ˜¯é æ¸¬å‡ºçœŸæ­£ Betweenness Centrality çš„å€¼ï¼Œè€Œæ˜¯"ç›¸å°"çš„æ’åå³å¯ã€‚å› æ­¤æŠŠæ¯å°é‚Š$(i, j)$ ç¯€é»é æ¸¬å€¼çš„å·® $y_i-y_j$ èˆ‡å¯¦éš›å€¼çš„å·® $b_i-b_j$ ä»£é€² [binary cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss) è£¡å¾—åˆ° loss å€¼ã€‚

:::warning
å¯¦éš›åŸ·è¡Œæ™‚ç™¼ç¾ç®—å‡ºä¾†çš„ loss å€¼æœƒéå¸¸å¤§ï¼Œæ¯æ¬¡è¨“ç·´ loss è®ŠåŒ–çš„å€¼å»å¾ˆå°‘ï¼Œå˜—è©¦éèª¿å¤§ learning rateï¼Œä½†æœƒå‡ºç¾æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ç­‰å•é¡Œï¼Œä¸å¤ªç¢ºå®šæ˜¯ä¸æ˜¯ loss function å‡ºéŒ¯é‚„æ˜¯å‰é¢æœ‰å“ªè£¡å‡ºå•é¡ŒğŸ˜¢
:::

----

### è¨“ç·´
#### Inductive Learning
è«–æ–‡ä¸­æ˜¯åˆ©ç”¨éµå®ˆ power-law åˆæˆçš„è¼ƒå°çš„åœ–ä¾†è¨“ç·´ï¼Œä»¥åœ¨çŸ­æ™‚é–“å…§å¯ä»¥è¨ˆç®—å‡ºæ­£ç¢ºçš„ Betweenness Centrality å€¼ï¼Œå†æŠŠè¨“ç·´å‡ºä¾†çš„æ¨¡å‹å¥—ç”¨åˆ°çœŸå¯¦çš„è¼ƒé¾å¤§çš„åœ–ä¸Šã€‚æˆ‘ç”¨ä½œæ¥­çµ¦çš„ Dataset ç¸½å…± 30 å¼µåœ–ä¾†è¨“ç·´ï¼Œæ¯å¼µåœ–å…±æœ‰ 5000 å€‹ç¯€é»ï¼Œå› ç‚ºä¸éœ€è¦å†å¦å¤–è¨ˆç®— Betweenness Centralityï¼Œèƒ½åŠ å¿«è¨“ç·´çš„é€Ÿåº¦ã€‚

:::warning
å‰å¹¾æ¬¡è¨“ç·´çš„æ™‚å€™æ¯å€‹ epoch å¾—åˆ°çš„æ­£ç¢ºç‡å¤§æ¦‚å¾ 10% ~ 40% ä¸ç­‰ï¼Œä½†è¨“ç·´åˆ°å¾Œé¢æ¼¸æ¼¸è¶¨å‘å¤§ç´„ 14%ã€‚
:::

:::danger
åœ¨æƒ³å› ç‚º Neighborhood Aggregation æ˜¯é€éçœ‹åœ–ä¸Šå„ç¯€é»çš„é„°å±…çš„ feature ä¾†èšåˆï¼Œè€Œæ¯æ¬¡é¤µçš„åœ–éƒ½ä¸ä¸€æ¨£ï¼Œå› æ­¤æœƒå¾—åˆ°ä¸ä¸€æ¨£çš„ featureï¼Œé‚£æœƒä¸æœƒé€™å€‹è¨“ç·´æ–¹æ³•ä¸¦ä¸èƒ½é©ç”¨ Inductive Learningï¼Ÿ
:::

---

## åƒè€ƒè³‡æ–™
- [ä»‹æ•¸ä¸­å¿ƒæ€§- ç¶­åŸºç™¾ç§‘ï¼Œè‡ªç”±çš„ç™¾ç§‘å…¨æ›¸ - Wikipedia](https://zh.wikipedia.org/wiki/%E4%BB%8B%E6%95%B0%E4%B8%AD%E5%BF%83%E6%80%A7)
- [Learning to Identify High Betweenness Centrality Nodes from Scratch: A Novel Graph Neural Network Approach](https://arxiv.org/abs/1905.10418v4)